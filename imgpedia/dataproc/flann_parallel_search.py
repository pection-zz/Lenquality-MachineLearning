from pyflann import *
import os, sys, cv2, threading, time
import numpy as np

'''
This script reads the descriptor bucket matrixes generated by flann_settings.py.
Then in parallel it searches each bucket in all the others, using FLANN
For each image, this script produces 160 candidates to be the 10-NN.
'''

if len(sys.argv) != 4:
	print "usage: python flann_parallel_search.py base_input base_output prefix"
	exit(1)
	
base_input = sys.argv[1]
base_output = sys.argv[2]
prefix = sys.argv[3]

folders = ["0","1","2","3","4","5","6","7","8","9","a","b","c","d","e","f"]

#receives the folder id to read the data and build up the index
#then it loads the targets and performs the knn-search
def knn_search(folder_id, data, k=10):
	flann = FLANN()
	dataset = data[folder_id]
	set_distance_type('manhattan')
	params = flann.build_index(dataset, algorithm="autotuned", target_precision=0.9);
	flann.save_index(os.path.join(base_output, folder_id+"_"+prefix+"_"+"index.txt"))
	for folder in folders:
		print "searching %s in %s" % (folder, folder_id)
		testset = data[folder]
		result, dists = flann.nn_index(testset, k, checks=params["checks"]);
		np.save(open(os.path.join(base_output, folder+"_"+folder_id+"result.txt"), "w"), result)
		np.save(open(os.path.join(base_output, folder+"_"+folder_id+"distances.txt"), "w"), dists)
		
#setup the data
data = {}
start = time.time()
print "gathering data"
for folder in folders:
	path = os.path.join(base_input, folder)
	dirs = []
	for (p, dirnames, filenames) in os.walk(path):
		dirs.extend(sorted(dirnames))
		break
	first = True
	for dir in dirs:
		matrix = os.path.join(path, dir,  dir+".npy")
		if first:
			dataset = np.load(matrix)
			first = False
		else:
			d = np.load(matrix)
			dataset = np.vstack((dataset, d))
	data[folder] = dataset

end_data = time.time()	
threads = []
print "launching threads"
for folder in folders:
	t = threading.Thread(target=knn_search, args=(folder, data))
	threads.append(t)
	t.start()
for thread in threads:
	t.join()
end_search = time.time()
fileout = open(prefix+"_search.txt", "w")
txt = "elapsed data time: %.4f, elapsed search time: %.4f, total time: %.4f" % (end_data-start, end_search-end_data,end_search-start)
fileout.write(txt)
fileout.close()